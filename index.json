[{"content":"","date":"2025 January 27","externalUrl":null,"permalink":"/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"2025 January 27","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":" CUDA 简介 # 引用自wiki:\nCUDA（Compute Unified Device Architecture，统一计算架构）是由英伟达NVIDIA所推出的一种软硬件集成技术，是该公司对于GPGPU的正式名称。透过这个技术，用户可利用NVIDIA的GPU进行图像处理之外的运算.\n简单来说,就是让显卡能够运行除了图形相关运算外,也可以作为计算卡加速其他的可编程计算,包括 AI 相关的推理.\n由于 GPU 计算单元远超 CPU ,所以大多数 AI 模型需要使用 GPU 支持以达到可接受的 训练/推理 时间.现行的主流深度学习框架 (torch,tensorflow) 对CUDA的适配最为良好和易用,因此推荐初学者和 AI 一般用户使用 Nvidia 显卡.\n从 CUDA 到 python # 底 层 | GPU 硬件 实际负责运算的`基本硬件` | | 驱动程序 操作系统层,直接指挥硬件的工作,图形渲染和科学计算都需要的`基本软件` | | CUDA 工具包 提供基本可编程性,高级用户可以通过 CUDA 编程实现GPU计算 | | Conda 环境 隔离python环境,使得 同一操作系统/环境 下切换不同python的需求更方便 | | python 深度学习框架的依赖语言 | ↓ 深度学习框架 提供高可编程的 AI 框架 顶 层 接下来让我们举例说明其中的兼容性\nGPU 硬件 # 依据显卡架构提供支持CUDA版本的上限 (下面表格摘自wiki百科,更详细的情况请查阅英文wiki,中文更新略有延迟)\nCUDA 版本 支持的计算能力 微架构 备注 1.0[10] 1.0 – 1.1 Tesla - 1.1 1.0 – 1.1+x Tesla - 2.0 1.0 – 1.1+x Tesla - 2.1 – 2.3.1[11][12][13][14] 1.0 – 1.3 Tesla - 3.0 – 3.1[15][16] 1.0 – 2.0 Tesla, Fermi - 3.2[17] 1.0 – 2.1 Tesla, Fermi - 4.0 – 4.2 1.0 – 2.1+x Tesla, Fermi - 5.0 – 5.5 1.0 – 3.5 Tesla, Fermi, Kepler - 6.0 1.0 – 3.5 Tesla, Fermi, Kepler - 6.5 1.1 – 5.x Tesla, Fermi, Kepler, Maxwell 最后支持计算能力 1.x (Tesla) 的版本 7.0 – 7.5 2.0 – 5.x Fermi, Kepler, Maxwell - 8.0 2.0 – 6.x Fermi, Kepler, Maxwell, Pascal 最后支持计算能力 2.x (Fermi) 的版本；GTX 1070Ti 不受支持 9.0 – 9.2 3.0 – 7.2 Kepler, Maxwell, Pascal, Volta Pascal GTX 1070Ti 不受 CUDA SDK 9.0 支持，但受 CUDA SDK 9.2支持 10.0 – 10.2 3.0 – 7.5 Kepler, Maxwell, Pascal, Volta, Turing 最后支持计算能力 3.x (Kepler) 的版本；CUDA SDK 10.2 是最后能用于 macOS 的官方版本，在未来的版本中 macOS 将不被支持 11.0 – 3.5 - 8.6 Maxwell, Pascal, Volta, Turing, Ampere - 在选择硬件时,请结合您项目的实际需求,请特别注意, CUDA 虽然有较好的兼容性设计,但是仍然存在较新显卡不能支持低版本 CUDA 的情况(详细情况请查询 wiki 的 CUDA 各版本算力支持范围)\nGPU 驱动 # 依据显卡nvida官网上选择,如果您没有特殊需求,建议使用最新版本驱动 安装完成后,使用nvidia-smi 即可查看 GPU 状况.\n+-----------------------------------------------------------------------------+ | NVIDIA-SMI 526.98 Driver Version: 526.98 CUDA Version: 12.0 | |-------------------------------+----------------------+----------------------+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA P106-100 TCC | 00000000:10:00.0 Off | N/A | | 33% 50C P0 29W / 120W | 8MiB / 6144MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ NVIDIA-SMI 为 Nvidia-smi 版本 Driver Version 为驱动版本 CUDA Version 为该驱动支持的 CUDA 最高版本 CUDA 版本 # 请依据项目提供的条件和自身驱动选择合适的 CUDA 版本,如果项目未指明,请根据 python/torch/tf 框架版本结合项目时间推测,通常可使用 CUDA 11.x 版本\n您可以通过下面的方法安装cuda\n直接安装,通过 nvidia 官网安装相应版本的 CUDA 工具包\n通过conda安装 ( 如果您正在服务器上,请考虑使用非 root 安装 miniconda 方法 ), 这里以 pytorch 为例\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia 您可以参考所需的学习框架版本,安装合适的 CUDA. 也可以在相关学习框架的官网上查找适配的 CUDA 版本\nconda安装 # 推荐使用 miniconda\npython 和相关包安装 # 略\n","date":"2025 January 27","externalUrl":null,"permalink":"/posts/cuda%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E7%AE%80%E4%BB%8B/","section":"","summary":"","title":"CUDA与深度学习环境简介","type":"posts"},{"content":" 命令处理一般流程 # 本项目以 Linux Bash 和 Huawei VRP 为例，推测 VRP CLI 可能的实现方法\n流程简图 # graph TB A[用户输入] --\u0026gt; B[命令搜索] B --\u0026gt; C{定位命令} C -- 定位成功 --\u0026gt; d[通知对应模块] C -- 定位失败 --\u0026gt; e[返回错误] d --\u0026gt; f[输出模块处理结果] f --\u0026gt; g(结束) e --\u0026gt; g 目标 # 在操作系统中，命令的处理和下发单独由一个模块进行处理。无论是命令行（CLI）和图形界面（GUI）都负责接受用户输入，并将用户输入传递给对应的功能模块。命令处理中不应该直接干涉功能，并且能够合理处理用户的复杂输入（例如脚本语言，命令补全）,传递给多个模块协同运行。\n通用操作系统的命令处理 # 以 Linux Bash 为例 # 通用操作系统的特点是个体间差异性较大，用户输入和使用场景较为复杂，面临对应功能模块的不确定性较大。因此，通用OS的命令行处理需要更多的考虑模块化和健壮性，对外提供合理的安装注册接口，对内预设各种可能的错误和异常情况\nBash 师出 UNIX sh ，在 Linux 中，应用程序通常以可执行文件的形式存在，而在 bash 识别过程中，会先判断键入命令是否是别名，如果是就进行替换，然后检查是否是bash的内部命令，例如cd、exit等。当然，内部命令也分为特殊内部命令，shell函数和普通内部命令，这部分负责脚本的识别和调整 Bash 自身的状态。 接着搜索PATH中指定的环境变量，查找与输入匹配的可执行文件。\n在 Linux 中，Bash 的搜索路径由环境变量 PATH 决定。 Bash 通过搜索这些可执行文件，将其余的命令参数传递给实现功能的模块。Bash 还有很多配置文件，由于历史原因，兼容性问题，分级管理设计，灵活性设计等， Bash 通常逐级读取系统级配置/etc/profile,用户级配置 ~/.bashrc和其他层级的配置文件。\n通过以上设计，我们可以发现 Bash 是与功能模块解耦的，安装功能只需要添加对应的搜索路径或者将二进制程序移动到搜索路径中即可。 Bash 本身也通过 fork-exec 等方式唤起其他可执行程序，保证自身的健壮性。但是，这样的设计无法天然支持对某个模块参数的补全，也无法进行模糊匹配，因为无法确定具体的可执行文件，将参数传递。在效率上 ，尽管 Bash 通过哈希等方式尝试加速曾经执行过命令的查找，也无法确保在命令类型/搜索路径中可执行文件数量较大，或是冷执行时的效率。\n可以使用如下脚本在 /usr/bin下生成5万个可执行文件\n#!/bin/bash for i in $(seq 1 50000); do echo -e \u0026#34;#!/bin/bash\\necho \\\u0026#34;This is file $i\\\u0026#34;\u0026#34; \u0026gt; file_$i.sh chmod +x file_$i.sh done echo \u0026#34;Generated.\u0026#34; 然后试试按Tab的响应速度，如果是对 file_试图补全的话，大概有1秒左右的延迟。\n正好最近在搞 huawei 的交换机，下面就以其网络操作系统为例介绍定制下的设计方案。\n定制操作系统的命令处理-以 Huawei VRP 设计为参考 # 设计目标 # 前面说明了通用操作系统的特点\u0026quot;解耦合，模块化，变化多\u0026quot;，那么定制操作系统则恰好与之相反，通常系统不会有太大的变化，但是本身具有相当程度的复杂性，需要命令能够快速查找和下发。所以，定制操作系统的CLI通常作为系统的某个核心模块进行编程开发，与其他模块之间有更高效的通信和协作方式。\n特点 # 命令分类视图化 支持不完整输入 支持命令帮助和补全 支持立即下发模式和延迟生效模式 其中，支持不完全输入和命令帮助是定制系统的重要功能。\n[Huawei]interface Ethernet0/0/0 [Huawei-Ethernet0/0/0]ip address 192.168.57.10 24 [Huawei-Ethernet0/0/0]i a 192.168.57.11 24 模型设计: 命令树 # （推测结果，不代表实际代码，我也不是 Huawei 员工）\n举例说明：\ngraph LR A[视图根节点\\n进入视图命令] --\u0026gt; B[token1, settings] B --\u0026gt; C[token3, apply] A --\u0026gt; D[token2, ip] D --\u0026gt; E[token4, address] D --\u0026gt; F[token5, mask] E --\u0026gt; G[token6 ,\u0026lt;字符串IP\u0026gt;] F --\u0026gt; G 我们以最简单命令树模型进行举例,上面这张图表示了某个视图下命令 settings apply 和 ip { address | mask } \u0026lt;IP\u0026gt; 的逻辑结构。 注意，这里仅代表token的逻辑结构，在命令树或内存中的实际表现可能有一定的出入。命令行格式和基本情况可以参考 Huawei 的命令手册 和 使用指南\n定义命令 # 对于交换机系统中的每个模块，可以定义该模块所需的所有token（参数），例如模块需要 settings apply 和 ip { address | mask } \u0026lt;IP\u0026gt; 两条命令，即定义6个token。 这些token按一定的顺序组成一条完整的命令\np[3] = new Token(\u0026#34;ip\u0026#34;,modeID) p[4] = new Token(\u0026#34;address\u0026#34;,modeID) p[5] = new Token(\u0026#34;mask\u0026#34;,modeID) p[6] = new IPclass cmd2.head = p[3] b[1] = new branch(p[4],p[5]) p[3].addNext(b[1]) p[4].addNext(p[6]) p[5].addNext(p[6]) 当然，设计系统的时候可以考虑提供更高等级的封装，将线索的具体解析移动到相应的模块处理,例如下面这种方法：\ncmd2.set(\u0026#34;p3 { p4 | p5 } p6\u0026#34;) /* * 然后函数调用相应模块生成命令树 * 这部分工作在初始化时完成，线索处理模块可以使用自动机完成（类似于编译器） */ 设置属性，合并命令子树 # 如果命令在现在和将来都不多的情况下，可以考虑使用两棵命令树\n对于每一个元素，可以添加帮助函数，在帮助操作命中该元素时，可以根据给定的前项元素给出提示输出 (如果元素的帮助含义唯一，也可以不需要识别帮助前项元素的来源)\np[3] = new Token(\u0026#34;ip\u0026#34;,modeID,\u0026#34;helpIP\u0026#34;) p[4] = new Token(\u0026#34;address\u0026#34;,modeID,addressHelpInfo) getHelp(vector UserInput){ if(UserInput.empty()) return this-\u0026gt;helpInfo[0]; switch(vector.end()-1){ case \u0026#34;ip\u0026#34;: return this-\u0026gt;helpInfo[1]; } } 为了区分命令对延迟生效模式和立即生效模式的支持程度，可以使用标记，并添加分支节点的数据结构\n请注意，在区别延迟生效模式和立即生效模式时，在后续的命令树合并过程中可能出现问题，需要提前设计好数据结构。 以下是举例说明\n命令 A B C 支持延迟生效 命令 A B D 不支持 命令 E F D 支持延迟生效 graph LR id1(A)--\u0026gt;id2(B)--\u0026gt;id3(C) id4(E)--\u0026gt;id5(F)--\u0026gt;id6(D) id7(A)--\u0026gt;id8(B)--\u0026gt;id9(D) style id1 fill:#090,stroke:#333,stroke-width:4px style id2 fill:#090,stroke:#333,stroke-width:4px style id3 fill:#090,stroke:#333,stroke-width:4px style id4 fill:#090,stroke:#333,stroke-width:4px style id5 fill:#090,stroke:#333,stroke-width:4px style id6 fill:#090,stroke:#333,stroke-width:4px style id7 fill:#990,stroke:#333,stroke-width:4px style id8 fill:#990,stroke:#333,stroke-width:4px style id9 fill:#990,stroke:#333,stroke-width:4px 合并后，节点D的属性产生冲突\ngraph LR id1(A)--\u0026gt;id2(B)--\u0026gt;id3(C) id4(E)--\u0026gt;id5(F)--\u0026gt;id6(D 产生冲突) id2(B)--\u0026gt;id6 style id1 fill:#090,stroke:#333,stroke-width:4px style id2 fill:#090,stroke:#333,stroke-width:4px style id3 fill:#090,stroke:#333,stroke-width:4px style id4 fill:#990,stroke:#333,stroke-width:4px style id5 fill:#990,stroke:#333,stroke-width:4px style id6 fill:#900,stroke:#333,stroke-width:4px 所以需要对命令的来源进行判断，以便能够加上标记。在这里我们引入新的分支节点，令其承担标记的工作\ngraph LR id1(A)--\u0026gt;id2(B)--\u0026gt;id7(B 分支1)--\u0026gt;id3(C) id2(B)--\u0026gt;id8(B 分支2)--\u0026gt;id6(D) id4(E)--\u0026gt;id5(F)--\u0026gt;id9(F 分支2)--\u0026gt;id6(D) style id7 fill:#090,stroke:#333,stroke-width:4px style id8 fill:#090,stroke:#333,stroke-width:4px style id9 fill:#990,stroke:#333,stroke-width:4px 这样就能分清楚D元素的命令来源是否需要支持延迟生效了。但是此时引入一个新的问题：如果D不是命令的终结元素，可能产生再次产生属性冲突。当然，功能模块的命令本身设计合理的情况下这种现象会被避免，不过我们在设计命令树框架时应该考虑这种情况，所以还需要添加终结节点\n再引入一条命令 A B D G\ngraph LR id10(A)--\u0026gt;id11(B)--\u0026gt;id12(D)--\u0026gt;id13(G) 合并后\ngraph LR id1(A)--\u0026gt;id2(B)--\u0026gt;id7(B 分支1)--\u0026gt;id3(C) id2(B)--\u0026gt;id8(B 分支2)--\u0026gt;id6(D) id4(E)--\u0026gt;id5(F)--\u0026gt;id9(F 分支2)--\u0026gt;id6(D) id6(D)--\u0026gt;id11(D 分支1)--\u0026gt;id10(G) id6(D)--\u0026gt;id98(end) id3(C)--\u0026gt;id99(end) id10(G)--\u0026gt;id97(end) style id7 fill:#090,stroke:#333,stroke-width:4px style id8 fill:#090,stroke:#333,stroke-width:4px style id11 fill:#090,stroke:#333,stroke-width:4px style id9 fill:#990,stroke:#333,stroke-width:4px 对于帮助函数而言，还有更复杂的情况需要考虑 再引入一条命令 E F D H 支持延迟模式\ngraph LR id10(E)--\u0026gt;id11(F)--\u0026gt;id12(D)--\u0026gt;id13(G) 合并后\ngraph LR id1(A)--\u0026gt;id2(B)--\u0026gt;id7(B 分支1)--\u0026gt;id3(C) id2(B)--\u0026gt;id8(B 分支2)--\u0026gt;id6(D) id4(E)--\u0026gt;id5(F)--\u0026gt;id9(F 分支2)--\u0026gt;id6(D) id6(D)--\u0026gt;id11(D 分支1)--\u0026gt;id10(G) id6(D)--\u0026gt;id12(D 分支2)--\u0026gt;id13(H) id6(D)--\u0026gt;id98(end) id3(C)--\u0026gt;id99(end) id10(G)--\u0026gt;id97(end) id13(H)--\u0026gt;id96(end) style id7 fill:#090,stroke:#333,stroke-width:4px style id8 fill:#090,stroke:#333,stroke-width:4px style id11 fill:#090,stroke:#333,stroke-width:4px style id12 fill:#990,stroke:#333,stroke-width:4px style id9 fill:#990,stroke:#333,stroke-width:4px 这样设计的话，就需要在分支节点中记录上一个合法元素了，记住来源以免判断错误。或是采用染色法，复用命令元素，但是命令的跟踪路径染色\ngraph LR id1(A)--\u0026gt;id2(B)--\u0026gt;id7(B 分支1)--\u0026gt;id3(C) id2(B)--\u0026gt;id8(B 分支2)--\u0026gt;id6(D) id4(E)--\u0026gt;id5(F)--\u0026gt;id9(F 分支2)--\u0026gt;id6(D) id6(D)--\u0026gt;id11(D 分支1)--\u0026gt;id10(G) id6(D)--\u0026gt;id12(D 分支2)--\u0026gt;id13(H) id6(D)--\u0026gt;id98(end) id3(C)--\u0026gt;id99(end) id10(G)--\u0026gt;id97(end) id13(H)--\u0026gt;id96(end) style id1 fill:,stroke-width:4px style id2 fill:,stroke-width:4px style id3 fill:,stroke-width:4px style id6 fill:,stroke-width:4px style id10 fill:,stroke-width:4px style id7 fill:#090,stroke:#333,stroke-width:4px style id8 fill:#090,stroke:#333,stroke-width:4px style id11 fill:#090,stroke:#333,stroke-width:4px style id12 fill:#990,stroke:#333,stroke-width:4px style id9 fill:#990,stroke:#333,stroke-width:4px 在后续的匹配过程中，待过滤元素需要和上一级保持一致。有多种元素的话，需要刷新为和上一级一致的唯一颜色（激活法）。\n另外也可以采用只合并公共前缀，不合并后缀的方法。这样确定的子树就是唯一的。不过内存的消耗会更大。对于上面的情况，可以改写为：\ngraph LR id1(A)--\u0026gt;id2(B)--\u0026gt;id51(无属性分支)--\u0026gt;id3(C) id2(B)--\u0026gt;id50(无属性分支)--\u0026gt;id6(D) id4(E)--\u0026gt;id5(F)--\u0026gt;id20(D) id20(D)--\u0026gt;id53(无属性分支)--\u0026gt;id11(D 分支1)--\u0026gt;id10(G) id20--\u0026gt;id54(无属性分支)--\u0026gt;id9(F 分支2)--\u0026gt;id95(end) id6(D)--\u0026gt;id12(D 分支1)--\u0026gt;id13(H) id6(D)--\u0026gt;id8(D 分支2)--\u0026gt;id98(end) id3(C)--\u0026gt;id7(C 分支1)--\u0026gt;id99(end) id10(G)--\u0026gt;id97(end) id13(H)--\u0026gt;id96(end) style id7 fill:#090,stroke:#333,stroke-width:4px style id8 fill:#090,stroke:#333,stroke-width:4px style id11 fill:#090,stroke:#333,stroke-width:4px style id12 fill:#990,stroke:#333,stroke-width:4px style id9 fill:#990,stroke:#333,stroke-width:4px 在合并子树时，将前置分支节点的延迟生效属性置空。应当确保，属性标签只能存在于该命令分支上的最后一个分支节点，其余都应该置空。 但是还有一种情况，在对某个分支属性进行帮助查询的时候，可能会不知道后续。\ngraph LR id4(E)--\u0026gt;id5(F)--\u0026gt;id20(D) id20(D)--\u0026gt;id53(无属性分支)--\u0026gt;id10(G) id20--\u0026gt;id9(F 分支2)--\u0026gt;id45(H)--\u0026gt;id95(end) id10(G)--\u0026gt;id11(G 分支1)--\u0026gt;id97(end) id10(G)--\u0026gt;id19(G 分支2)--\u0026gt;id46(I)--\u0026gt;id90(end) style id11 fill:#090,stroke:#333,stroke-width:4px style id19 fill:#090,stroke:#333,stroke-width:4px style id9 fill:#990,stroke:#333,stroke-width:4px 例如对 D 进行帮助操作的时候会需要判断是否显示 G 但是 G 本身是一个分支节点，就需要继续向后查询，直到判断出某个具体的分支。 这里就需要注意了，向后查询确实能够保证搜索结果的正确性，但是会大大降低搜索效率。如果某个分支的子分支非常多，子节点层数深，那么搜索将会耗费大量的时间，还有可能搜索到最后结果也为空。\n对于这种情况，一个解决办法是在初始化命令树时，对可能存在延迟生效属性的分支进行染色，相当于通过颜色在逻辑上区分不同环境下的命令树，而在实际内存中复用同一棵命令树。具体是实现是在合并命令树时，递归地染色挂载点及其父级分支节点。在检索时，如果发现该节点被染色，说明其分支中存在支持延迟生效模式的命令元素组合，深入搜索；否则放弃整棵子树。\ngraph LR id4(E)--\u0026gt;id5(F)--\u0026gt;id20(D) id20(D)--\u0026gt;id53(无属性分支)--\u0026gt;id10(G) id20--\u0026gt;id52(无属性分支)--\u0026gt;id45(H)--\u0026gt;id54(H 分支1)--\u0026gt;id95(end) id45(H)--\u0026gt;id9(H 分支2)--\u0026gt;id94(end) id10(G)--\u0026gt;id11(G 分支1)--\u0026gt;id97(end) id10(G)--\u0026gt;id19(G 分支2)--\u0026gt;id46(I)--\u0026gt;id90(end) style id11 fill:#090,stroke:#333,stroke-width:4px style id54 fill:#090,stroke:#333,stroke-width:4px style id19 fill:#990,stroke:#333,stroke-width:4px style id20 fill:#a0a,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 style id10 fill:#a0a,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 style id9 fill:#090,stroke:#333,stroke-width:4px 挂载完整命令树 # 在定义完元素，设计好命令之后，CLI在初始化命令树时，各个模块向CLI注册自己的命令行，即CLI根据配置文件调用或各模块自行初始化。\n// 某功能模块 class Mod{ public: Token** p = new Token*[10] Mod(){ p[3] = new Token(\u0026#34;ip\u0026#34;,modeID) p[4] = new Token(\u0026#34;address\u0026#34;,modeID) p[5] = new Token(\u0026#34;mask\u0026#34;,modeID) p[6] = new IPclass p[7] = new IPv6class cmd1.set(\u0026#34;p3 { p4 | p5 } p6\u0026#34;) cmd2.set(\u0026#34;p3 { p4 | p5 } p7\u0026#34;) } } 每个模块拥有其自身的命令子树，每个视图下拥有多个模块，形成该视图下的命令树\ngraph LR UserView(用户视图) SystemView(系统视图) CmdSystem(进入命令 System) CmdInterface(进入命令 Interface) CmdLine(进入命令 Line) LineView(Line 视图) Interface(Interface 视图) LineViewTree1(Line 视图命令子树1) LineViewTree2(Line 视图命令子树2) InterfaceTree1(Interface 视图命令子树1) InterfaceTree1(Interface 视图命令子树2) UserView--\u0026gt;CmdSystem--\u0026gt;SystemView SystemView--\u0026gt;CmdInterface--\u0026gt;Interface SystemView--\u0026gt;CmdLine--\u0026gt;LineView Interface--\u0026gt;InterfaceTree1 Interface--\u0026gt;InterfaceTree2 LineView--\u0026gt;LineViewTree1 LineView--\u0026gt;LineViewTree2 当然，具体的进入命令实现需要依赖我们后面设计的命令检索与实现。\n命令树的检索、自动补全 # 当命令树构建完全后，需要处理用户输入来找到正确的命令路径，这里我们以只合并公共前缀，使用染色方法区分模式的，命令树说明检索的实现\ngraph LR idE(E) idF(F) idD(D) idG(G) idH(H) idE--\u0026gt;idF--\u0026gt;idD idD--\u0026gt;id53(无属性分支)--\u0026gt;idG idD--\u0026gt;id52(无属性分支)--\u0026gt;idH--\u0026gt;id54(H 分支1)--\u0026gt;id95(end) idH--\u0026gt;id9(H 分支2)--\u0026gt;id94(end) idG--\u0026gt;id11(G 分支1)--\u0026gt;id97(end) idG--\u0026gt;id19(G 分支2)--\u0026gt;id46(I)--\u0026gt;id90(end) style id11 fill:#090,stroke:#333,stroke-width:4px style id54 fill:#090,stroke:#333,stroke-width:4px style id19 fill:#990,stroke:#333,stroke-width:4px style idD fill:#a0a,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 style idG fill:#a0a,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 style id9 fill:#090,stroke:#333,stroke-width:4px 假设用户输入为 E F D G I,CLI将会接受这一段输入，拆分为四个元素，向末尾添加\u0026lt;end\u0026gt;,依次检索\n取第一个输入 E 匹配该视图的根节点，将 E 加入过滤器\n取第二个输入 F 匹配过滤器中 E 的下级节点，二级过滤器中加入 F\n取第三个输入 D 匹配过滤器中 F 的下级节点，三级过滤器中加入 D\n当然，这里的多级过滤器在实际中可以设计为一个队列复用。\n取第四个输入 G 匹配过滤器中 D 的某一下级节点，丢弃子节点H,四级过滤器中加入 G\n取第五个输入 I 匹配过滤器中 G 的某一下级节点，丢弃子节点\u0026lt;end\u0026gt;,五级过滤器中加入 G\n取第六个输入 \u0026lt;end\u0026gt; 匹配过滤器中 I 下级节点，六级过滤器中加入\u0026lt;end\u0026gt;\n输入队列为空，过滤器结果合法，匹配完成\n此时可以调用每特殊元素的检查函数，例如IPv6class的输入IPv6是否合法，字符串是否超过最大长度等。对于延迟生效模式，只需要增加对染色的判断即可。 帮助的实现与之类似，输入队列为空后，依次输入过滤器中元素对应的帮助信息即可，自动补全实现类似，不再赘述。 如果过滤器为空或出现非法情况，返回当前队列位置处出错即可。\n命令消息传递 # 当命令树完成对用户输入的匹配后，需要将命令消息传递给相应的模块进行处理。通过匹配的最终元素内的ID确定该条命令应该被传送到视图下的哪一个模块。请注意。这里有一个问题，如果两个模块注册了“相同”的命令元素（指字符内容相同，但是模块ID不同）可以有两种处理办法：\n第一种是禁止相同的公共前缀（例如首元素必须不同），这样能确保命令一开始就会在正确的子树中匹配；第二种是合并所有相同的前缀字符串，并将其ID设置为公共，所传送的模块由最后一个匹配的命令元素模块ID确定。\n确定完传递给哪一个模块后，CLI只需将匹配到的命令以元素ID组的形式发给功能模块，由功能模块自行根据含命令元素ID的消息进行处理。\n其他 # 因为命令树的初始化是一个较为耗时的工作，可以将命令树作为守护进程使用，前端进程负责与其通信或者与其的fork子进程通信。模块的具体使用可以用父子进程消息传递，也可以直接使用函数堆栈的设计方案。具体操作例如会话，进程组，字符设备，伪终端等概念在抽象的方案设计中就不再详述。\n其他模型设计: Bash-redis # 除了将 CLI 与系统整合在一起外，还可以选择将CLI分离得更多一些。利用现有组件，可以采取 Bash + redis 的方法来组合解析命令树。 其中，Bash可以结合shell以及python脚本实现对命令的分类和解析，将内存中的命令树转化为文件系统的目录树，视图作为文件夹，帮助和依赖脚本和python实现（脚本和python可以又由其他工具自动生成）。其他功能模块打包为可执行文件，结合搜索的环境变量位置，实现对命令的分类处理。对外可以屏蔽本文件系统，也使用类似 VRP 的 shell。\n系统内部通信使用内存数据库（以redis为例），可以将参数和系统状态都放入内存数据库中，各个模块自行从中读取，消息的传递可以借由进程间通信，也可以使用内存数据库作为消息总线，不过这样的风险可能是速度较慢，需要合理规划，数据平面和控制平面分开处理。 这样的设计还有利于分布式和高可用，服务挂掉只需要重启某一个容器即可。\n","date":"2024 December 17","externalUrl":null,"permalink":"/posts/%E5%AE%9A%E5%88%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%91%BD%E4%BB%A4%E6%A0%91%E5%AE%9E%E7%8E%B0/","section":"","summary":"","title":"定制系统的命令树实现","type":"posts"},{"content":" 前言 # 基于N2S模型进行了修改\n源码地址\nreadme\n增强部分 # 我们增强了 N2S 模型的损失函数部分\n原始损失函数如下：\n$$ E_ {x,y} |f(x)- y|^ {2} + E_ {x,y} |x- y|^ {2} = E_ {x} |f(x)- x|^ {2} + 2E_ {x} , y\u0026lt;f(x)-y,x-y $$\n其中，y代表不含噪声的图片（未知），x表示含噪声的图片，f(x) 表示经过模型处理的图片。\n基于等式：x和y的差距等于 x 和 f(x )的差距与 f(x) 和 y 的差距的和：\n$$ x-y=(x-f(x))+(f(x)-y) $$\n变形可得：\n$$ (x-y)-(f(x)-y) = x-f(x) $$\n平方：\n$$ E_ {x,y} || f(x)-y || - E_ {x,y} || (x-y) ^ {2} || = E_ {x} || f(x)-x || $$\n结合平方差展开后移项，作者得到了以上等式。在经过复杂的数学方式证明后，作者得到了如下不等式，其中m表示图片中的像素多少，σ表示噪声的标准差：\n$$ E_{x,y} ||f(x)- y|^ {2} + E_ {x}y||x- y|^ {2} \\leqslant E_ {x}|f(x)- x||^ {2} + 2m \\sigma E_{J} [ \\frac{E||f(x)_ {1} -f(xJ_{c})J||^ {2} }{|J|}] ^ { \\frac{1}{2}} $$\n通过等式，作者将未知的y全部放在了左边。从图像处理角度分析，不等式左边表示不含噪声的图片和原图的差距（恒定）和模型输出图片与不含噪声图片差距的平方和。右面第一项表示模型输出与输入的平方和，右边第二项则是J不变项。从数学角度分析，我们期望模型输出图片f(x)与不含噪声图片y的差距越小越好，因此我们希望不等式右边也要越小越好。其中σ由于要同时得到x和y才能得知，N2S方法计算中实际使用了1来代替它。为了减小计算量以及更简洁的表达公式，我们把等式右边同除m，并使用λ来代替2σ，把结果作为训练过程中的损失函数，得到以下:\n{% mathjax %} \\mathcal {L}(f) = \\frac{E_{x} || f(x)-x || ^ {2}}{m} + \\lambda _{inv}E_{J} [E_{x} \\frac {||f(x)_{J}-f(xJ_{c})_{J}|| ^{2}}{|J|}]^{1/2} {% endmathjax %} Noise2Same方法通过创新性的不等式，得到了很好的结果。但是，其中也存在着一些缺陷，比如标准差σ无法直接取得，需要进行估计。在一些标准差较为恒定但与1差距较大的数据集中（比如同一老相机拍摄的图片集中可能含有相似的σ），由于σ会影响损失函数，所以收敛效果可能并没有那么好。这时，我们可以将σ一起列入未知项中，卷积神经网络每次迭代时将σ一起进行更新，这样可以有效提高模型的迭代速度。经过我们小组的研究，发现 $ E_{x}|| f(x) - x||^{2} $ 项约为 $ \\sigma ^{2} $ 。由此，我们对n2s的损失函数进行了优化，使用$2 \\sqrt {Ex||x-f(x)||^ {2}} $ 来代替λ。由此，现在的损失函数变为：\n{% mathjax %} \\mathcal {L}(f) = E_ {x} || f(x)- x||^ {2} /m + 2\\sqrt {Ex||x-f(x)||^ {2}} [ E_{x} ||f(x)_{J} - f(xJ_{c})_{J}||^ {2} / {|J|}]^{1/2} {% endmathjax %} 经过对损失函数的改进，算法的收敛速度和整体性能得到了一定程度的提升.\nREADME # 项目介绍 # 本项目是基于自监督去噪模型Noise2Same的改进，通过优化损失函数，提升了模型收敛速度和去噪效果.\n使用效果 # 去噪效果\n模型特点 # 介绍 # 本模型依赖于Noise2Same(下称N2S)模型的训练方法，在训练一定次数的N2S上继续训练得到Noise2SamePro(下称N2SP)，使得其收敛速度更快，图片去噪效果更好。\n与N2S模型的对比\n方法 PSNR 传统方法 Input 20.19 NLM 22.73 有监督 Noise2True 29.06 Noise2Noise 28.86 自监督 Noise2Same 27.09 Noise2SamePro 27.10 数据集 # 我们使用了N2S模型提供的数据集,有关数据集构建和数据源的更多详细信息可以在 Denoising_data 下找到.\n检查点 # 我们提供基于上述数据集训练9万轮次的检查点，请点击这里下载.\n使用方法 # 环境 # python==3.7.2 tensorflow==1.15 scipy scikit-image tifffile gdown opencv-python numpy matplotlib PIL 目录树 # Noise2SamePro ├─ test.py * ├─ test_pro.py * ├─ network_configure.py ├─ models.py * ├─ requirements.txt ├─ basic_ops.py ├─ train.py * ├─ train_pro.py * ├─ utils │ ├─ train_utils.py │ ├─ evaluation_utils.py │ └─ predict_utils.py ├─ README.md ├─ network.py ├─ resnet_module.py ├─ Denoising_data ------------------- 默认的数据集文件夹 ├─ trained_model -------------------- 默认的模型存放位置 └─ test_single.py 您需要关注的是标记有*的文件/文件夹，后文会介绍通过修改这些文件进行自定义训练的方法.本项目源代码中同时含有N2S模型与N2SP模型，其中test.py,train.py是有关N2S模型的训练与测试部分；test_pro.py,train_pro.py是有关N2SP的训练与测试部分.\n下文介绍N2SP的测试与训练方法，N2S与之类似(您也可参照N2S原仓库).\n使用 # 在test_single.py中，请修改如下部分进行单张图片的去噪\npicture = \u0026#39;man/\u0026#39; # Adjust path of the picture you want to test model_dir = \u0026#39;N2S_PRO\u0026#39; # Adjust your model path test_single(\u0026#39;test_single/\u0026#39; + picture + \u0026#39;original_image.png\u0026#39;, model_dir, \u0026#39;test_single/\u0026#39; + picture + \u0026#39;denoised_image.png\u0026#39;) 执行以下命令并等待即可\npython test_single.py 测试 # 如果您使用我们提供的检查点和默认路径，请将其放在trained_model/文件夹下(没有则请新建)\n确保test_pro.py文件中检查点路径与数据集路径正确.\nmodel_dir = \u0026#39;N2S_PRO-90000\u0026#39; # Adjust your model path data_dir = \u0026#39;Denoising_data/test/\u0026#39; model = Noise2Same(\u0026#39;trained_models/\u0026#39;, model_dir, dim=2, in_channels=1) 执行以下命令并等待即可\npython test_pro.py 训练 # N2SP的训练方法是：\n对于T次训练，采取(T-a)次对于N2S的训练和a次对N2SP的训练\n确保路径正确后，修改train_pro.py\nmodel_dir = \u0026#39;N2S_PRO-8000\u0026#39; # Set model checkpoints save path steps = 8000 # Set training steps sgm_loss = 1 # the default sigma is 1 model = Noise2Same(\u0026#39;trained_models/\u0026#39;, model_dir, dim=2, in_channels=1, lmbd=2*sgm_loss) model.train(X[..., None], patch_size=[64, 64], validation=X_val[..., None], batch_size=64, steps=steps-500) model = Noise2SamePro(\u0026#39;trained_models/\u0026#39;, model_dir, dim=2, in_channels=1) model.train(X[..., None], patch_size=[64, 64], validation=X_val[..., None], batch_size=64, steps=steps) 在上面的数值中，steps = 8000是总步数，sgm_loss = 1是N2S模型使用的参数(默认为1),steps=steps-500中的500是N2SP的训练次数\n即8000为 上述T;500为上述a.训练过程是进行7500次N2S的预训练，然后进行500次的附加训练.\n调整完毕后，执行以下命令并等待即可\npython train_pro.py 其他事项 # 贡献者 # 本项目是天津大学2021图像处理结课作业的去噪部分，由郭骐瑞，刘原驰，罗嘉杰，周欣宇，陈嘉强，李进共同完成\n","date":"2024 January 17","externalUrl":null,"permalink":"/posts/%E6%94%B9%E8%BF%9B%E5%8E%BB%E5%99%AA%E6%A8%A1%E5%9E%8B-noise2same/","section":"","summary":"","title":"改进去噪模型-Noise2Same","type":"posts"},{"content":"","externalUrl":null,"permalink":"/about/","section":"","summary":"","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]